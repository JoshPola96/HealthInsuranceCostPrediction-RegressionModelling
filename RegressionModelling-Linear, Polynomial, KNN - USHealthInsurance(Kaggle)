{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":949070,"sourceType":"datasetVersion","datasetId":514919}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshpeter96/regression-modelling-linear-polynomial-knn-ushins?scriptVersionId=191508222\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Insurance Charges Prediction","metadata":{"id":"Bq6U61fv5tKf"}},{"cell_type":"markdown","source":"## Introduction\nThis notebook explores the insurance dataset to build and evaluate different regression models. The dataset includes information such as age, sex, BMI, number of children, smoking status, and region, with the goal of predicting insurance charges.","metadata":{"id":"JTqSdy5fHnsK"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"id":"SXi_gva7BwbE","execution":{"iopub.status.busy":"2024-08-07T09:30:15.917189Z","iopub.execute_input":"2024-08-07T09:30:15.9176Z","iopub.status.idle":"2024-08-07T09:30:15.923319Z","shell.execute_reply.started":"2024-08-07T09:30:15.917562Z","shell.execute_reply":"2024-08-07T09:30:15.92193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Description\nThe dataset consists of the following features:\n- **age**: Age of the insured individual.\n- **sex**: Gender of the insured (Male/Female).\n- **bmi**: Body Mass Index of the insured.\n- **children**: Number of children/dependents covered by the insurance.\n- **smoker**: Whether the insured is a smoker (Yes/No).\n- **region**: Geographical region of the insured (Northwest, Southeast, Southwest, or Northeast).\n- **charges**: Insurance charges billed to the individual.\n\nThe dataset is analyzed to understand the relationships between these features and the insurance charges.","metadata":{"id":"xoHyYaxZ501s"}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ushealthinsurancedataset/insurance.csv')","metadata":{"id":"iER-S8K0AitS","execution":{"iopub.status.busy":"2024-08-07T09:30:15.92576Z","iopub.execute_input":"2024-08-07T09:30:15.926683Z","iopub.status.idle":"2024-08-07T09:30:15.943591Z","shell.execute_reply.started":"2024-08-07T09:30:15.926643Z","shell.execute_reply":"2024-08-07T09:30:15.942491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"VeZHtW7x39m4","outputId":"0faaf815-5247-4292-ed5a-faeafaf64251","execution":{"iopub.status.busy":"2024-08-07T09:30:15.945085Z","iopub.execute_input":"2024-08-07T09:30:15.94567Z","iopub.status.idle":"2024-08-07T09:30:15.960387Z","shell.execute_reply.started":"2024-08-07T09:30:15.94563Z","shell.execute_reply":"2024-08-07T09:30:15.959159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nIn this section, we perform exploratory data analysis to understand the dataset better. We will:\n- Check for missing values.\n- Visualize the distribution of numerical features.\n- Analyze correlations between numerical features.","metadata":{"id":"tMPAixM_6Dh1"}},{"cell_type":"code","source":"df.info()","metadata":{"id":"Atn_YLKuAnJI","outputId":"25750f49-c364-4e41-e3c6-0294b34f6915","execution":{"iopub.status.busy":"2024-08-07T09:30:15.962002Z","iopub.execute_input":"2024-08-07T09:30:15.962411Z","iopub.status.idle":"2024-08-07T09:30:15.978547Z","shell.execute_reply.started":"2024-08-07T09:30:15.962381Z","shell.execute_reply":"2024-08-07T09:30:15.977185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"id":"uuVK7psKAoBA","outputId":"10f0f601-dbb3-4bc2-b8c0-464523d13370","execution":{"iopub.status.busy":"2024-08-07T09:30:15.981388Z","iopub.execute_input":"2024-08-07T09:30:15.981751Z","iopub.status.idle":"2024-08-07T09:30:16.005976Z","shell.execute_reply.started":"2024-08-07T09:30:15.981715Z","shell.execute_reply":"2024-08-07T09:30:16.004636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"EXgXpQ6l3RAm","outputId":"6f8f2b5e-8240-47b0-898f-1d515e2b451e","execution":{"iopub.status.busy":"2024-08-07T09:30:16.007469Z","iopub.execute_input":"2024-08-07T09:30:16.007806Z","iopub.status.idle":"2024-08-07T09:30:16.016771Z","shell.execute_reply.started":"2024-08-07T09:30:16.007777Z","shell.execute_reply":"2024-08-07T09:30:16.015777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Distributions","metadata":{"id":"6CN9y2NW6Nnq"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(['age', 'bmi', 'children', 'charges']):\n    plt.subplot(2, 2, i + 1)\n    sns.histplot(df[col], kde=True)\n    plt.title(f'Distribution of {col}')\nplt.tight_layout()\nplt.show()","metadata":{"id":"sNhRdzHs3XsV","outputId":"67bad5c8-569c-4b3b-8597-2aaa75146ed9","execution":{"iopub.status.busy":"2024-08-07T09:30:16.017995Z","iopub.execute_input":"2024-08-07T09:30:16.018351Z","iopub.status.idle":"2024-08-07T09:30:17.419669Z","shell.execute_reply.started":"2024-08-07T09:30:16.018323Z","shell.execute_reply":"2024-08-07T09:30:17.418564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrix\nWe visualize the correlation matrix to understand the relationships between numerical features.","metadata":{"id":"YjtmN7Qo6ZC9"}},{"cell_type":"code","source":"df_encoded = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)\n\ncorr = df_encoded.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix with Encoded Categorical Variables')\nplt.show()","metadata":{"id":"roBoBkjSApIB","outputId":"f65c85c6-a623-4bc2-e01d-0bf6a61a1db4","execution":{"iopub.status.busy":"2024-08-07T09:30:17.421067Z","iopub.execute_input":"2024-08-07T09:30:17.421398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pairwise Plots for Features:","metadata":{"id":"T5xXjSsd6wjx"}},{"cell_type":"code","source":"sns.pairplot(df, hue='smoker')\nplt.show()","metadata":{"id":"dt0Q-Jt8AmJJ","outputId":"9481e5a9-839c-4cf9-df82-c0d4f410a61f","execution":{"iopub.status.idle":"2024-08-07T09:30:24.865226Z","shell.execute_reply.started":"2024-08-07T09:30:17.974967Z","shell.execute_reply":"2024-08-07T09:30:24.864155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\nBefore building the models, we need to preprocess the data. This involves:\n- Encoding categorical variables.\n- Scaling numerical features.","metadata":{"id":"mrVFNtgY7Avg"}},{"cell_type":"markdown","source":"### Encoding Categorical Variables\n","metadata":{"id":"wEKXpPRf7NtG"}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline","metadata":{"id":"h_JunTD37RYr","execution":{"iopub.status.busy":"2024-08-07T09:30:24.866465Z","iopub.execute_input":"2024-08-07T09:30:24.866779Z","iopub.status.idle":"2024-08-07T09:30:24.871674Z","shell.execute_reply.started":"2024-08-07T09:30:24.866751Z","shell.execute_reply":"2024-08-07T09:30:24.870772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define the preprocessing steps","metadata":{"id":"dD1xaiiA7XQ0"}},{"cell_type":"code","source":"X = df.drop('charges', axis=1)\ny = df['charges']\n\ncategorical_cols = ['sex', 'smoker', 'region']\nnumerical_cols = ['age', 'bmi', 'children']\n\nnumerical_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(drop='first')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"id":"sMLl2aPTiFth","execution":{"iopub.status.busy":"2024-08-07T09:30:24.87493Z","iopub.execute_input":"2024-08-07T09:30:24.875307Z","iopub.status.idle":"2024-08-07T09:30:24.889339Z","shell.execute_reply.started":"2024-08-07T09:30:24.875277Z","shell.execute_reply":"2024-08-07T09:30:24.88822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building and Evaluation\nWe build and evaluate different regression models to predict insurance charges. The models include Linear Regression, Polynomial Regression, and K-Nearest Neighbors","metadata":{"id":"DzhRc8s87x30"}},{"cell_type":"markdown","source":"### Linear Regression","metadata":{"id":"AlsrUpYn8cgU"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinear_reg_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', LinearRegression())\n])\n\nlinear_reg_pipeline.fit(X_train, y_train)\n\npred_linear_reg = linear_reg_pipeline.predict(X_test)","metadata":{"id":"rHQS3jeqF_CX","execution":{"iopub.status.busy":"2024-08-07T09:30:24.890568Z","iopub.execute_input":"2024-08-07T09:30:24.89114Z","iopub.status.idle":"2024-08-07T09:30:24.917991Z","shell.execute_reply.started":"2024-08-07T09:30:24.891103Z","shell.execute_reply":"2024-08-07T09:30:24.916914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluate the model","metadata":{"id":"VoydRgGq8wuN"}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nmse = mean_squared_error(y_test, pred_linear_reg)\nmae = mean_absolute_error(y_test, pred_linear_reg)\nr2score = r2_score(y_test, pred_linear_reg)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"R squared: {r2score}\")","metadata":{"id":"rw3nZ8u2GUoP","outputId":"fabbb54a-f312-4a73-9a34-1fea3b6710d6","execution":{"iopub.status.busy":"2024-08-07T09:30:24.919369Z","iopub.execute_input":"2024-08-07T09:30:24.919778Z","iopub.status.idle":"2024-08-07T09:30:24.928977Z","shell.execute_reply.started":"2024-08-07T09:30:24.919742Z","shell.execute_reply":"2024-08-07T09:30:24.927525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{"id":"koKr3bpvAW2b"}},{"cell_type":"code","source":"plt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, pred_linear_reg, color='blue', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('Linear Regression')\n\nplt.subplot(1, 3, 2)\nsns.scatterplot(x=pred_linear_reg, y=y_test - pred_linear_reg, color='blue', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"yCzz9QncAc-9","outputId":"3fe39505-7692-4904-9900-dd4dba75579f","execution":{"iopub.status.busy":"2024-08-07T09:30:24.930564Z","iopub.execute_input":"2024-08-07T09:30:24.930957Z","iopub.status.idle":"2024-08-07T09:30:25.515398Z","shell.execute_reply.started":"2024-08-07T09:30:24.930919Z","shell.execute_reply":"2024-08-07T09:30:25.514222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polynomial Regression","metadata":{"id":"Q4x0Gp2288U0"}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\n\npoly_reg_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('poly', PolynomialFeatures()),\n    ('model', LinearRegression())\n])\n\nparam_grid_poly = {\n    'poly__degree': [2, 3, 4],\n    'poly__include_bias': [True, False],\n    'model__fit_intercept': [True, False]\n}\n\ngrid_search_poly = GridSearchCV(poly_reg_pipeline, param_grid_poly, cv=5, scoring='neg_mean_squared_error', n_jobs = -1)\n\ngrid_search_poly.fit(X_train, y_train)\n\nbest_poly_model = grid_search_poly.best_estimator_\npoly_predictions = best_poly_model.predict(X_test)","metadata":{"id":"YSCD_r7UKBzt","execution":{"iopub.status.busy":"2024-08-07T09:30:25.516784Z","iopub.execute_input":"2024-08-07T09:30:25.517132Z","iopub.status.idle":"2024-08-07T09:30:28.466516Z","shell.execute_reply.started":"2024-08-07T09:30:25.517104Z","shell.execute_reply":"2024-08-07T09:30:28.464882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluating the results","metadata":{"id":"9FjC4HPu9k6L"}},{"cell_type":"code","source":"mse_poly = mean_squared_error(y_test, poly_predictions)\nmae_poly = mean_absolute_error(y_test, poly_predictions)\nr2_poly = r2_score(y_test, poly_predictions)\n\nprint(f\"Best Polynomial Regression Model Parameters: {grid_search_poly.best_params_}\")\nprint(f\"Best Score for Polynomial Regression: {-grid_search_poly.best_score_}\")\nprint(f\"Mean Squared Error: {mse_poly}\")\nprint(f\"Mean Absolute Error: {mae_poly}\")\nprint(f\"R squared: {r2_poly}\")","metadata":{"id":"0nE72RqKKnuK","outputId":"183b7d25-737d-4b64-dc95-aa71481aea0c","execution":{"iopub.status.busy":"2024-08-07T09:30:28.472754Z","iopub.execute_input":"2024-08-07T09:30:28.476434Z","iopub.status.idle":"2024-08-07T09:30:28.50399Z","shell.execute_reply.started":"2024-08-07T09:30:28.476368Z","shell.execute_reply":"2024-08-07T09:30:28.502252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{"id":"0gmvW4rQAjH3"}},{"cell_type":"code","source":"plt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, poly_predictions, color='green', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('Polynomial Regression')\n\nplt.subplot(1, 3, 2)\nsns.scatterplot(x=poly_predictions, y=y_test - poly_predictions, color='green', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('Polynomial Regression Residuals')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"PwlHqidgAlQ3","outputId":"5986cf08-6fab-42e0-9742-c51ed49da312","execution":{"iopub.status.busy":"2024-08-07T09:30:28.507308Z","iopub.execute_input":"2024-08-07T09:30:28.507786Z","iopub.status.idle":"2024-08-07T09:30:29.161172Z","shell.execute_reply.started":"2024-08-07T09:30:28.507745Z","shell.execute_reply":"2024-08-07T09:30:29.160026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neighbors","metadata":{"id":"Tfcdzzt193bM"}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n\nparam_grid_knn = {\n    'model__n_neighbors': [2, 3, 4, 5, 6, 7],\n    'model__metric': ['euclidean', 'manhattan'],\n    'model__weights': ['uniform', 'distance']\n}\n\ngrid_search_knn = GridSearchCV(knn_pipeline, param_grid_knn, cv=5, scoring='neg_mean_squared_error', n_jobs = -1)\n\ngrid_search_knn.fit(X_train, y_train)\n\nbest_knn_model = grid_search_knn.best_estimator_\nknn_predictions = best_knn_model.predict(X_test)","metadata":{"id":"1paQh1Y2MAsF","execution":{"iopub.status.busy":"2024-08-07T09:30:29.162985Z","iopub.execute_input":"2024-08-07T09:30:29.163433Z","iopub.status.idle":"2024-08-07T09:30:30.104509Z","shell.execute_reply.started":"2024-08-07T09:30:29.163394Z","shell.execute_reply":"2024-08-07T09:30:30.103403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluating the results","metadata":{"id":"gjULPNaN-_ic"}},{"cell_type":"code","source":"mse_poly = mean_squared_error(y_test, knn_predictions)\nmae_poly = mean_absolute_error(y_test, knn_predictions)\nr2_poly = r2_score(y_test, knn_predictions)\n\nprint(f\"Best Polynomial Regression Model Parameters: {grid_search_knn.best_params_}\")\nprint(f\"Best Score for Polynomial Regression: {-grid_search_knn.best_score_}\")\nprint(f\"Mean Squared Error: {mse_poly}\")\nprint(f\"Mean Absolute Error: {mae_poly}\")\nprint(f\"R squared: {r2_poly}\")","metadata":{"id":"4gqnpj9-mkoZ","outputId":"50de7cee-2c95-441a-bb1a-32bf0a260a4e","execution":{"iopub.status.busy":"2024-08-07T09:30:30.105948Z","iopub.execute_input":"2024-08-07T09:30:30.106312Z","iopub.status.idle":"2024-08-07T09:30:30.115818Z","shell.execute_reply.started":"2024-08-07T09:30:30.106282Z","shell.execute_reply":"2024-08-07T09:30:30.114607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{"id":"RmN6MtnuAtD_"}},{"cell_type":"code","source":"plt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, knn_predictions, color='red', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('K-Nearest Neighbors')\n\nplt.subplot(1, 3, 2)\nsns.scatterplot(x=knn_predictions, y=y_test - knn_predictions, color='red', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('K-Nearest Neighbors Residuals')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"--m7ekTDAxm-","outputId":"421b32ff-c312-4a57-971c-56b5096a924b","execution":{"iopub.status.busy":"2024-08-07T09:30:30.117232Z","iopub.execute_input":"2024-08-07T09:30:30.117549Z","iopub.status.idle":"2024-08-07T09:30:30.674133Z","shell.execute_reply.started":"2024-08-07T09:30:30.117522Z","shell.execute_reply":"2024-08-07T09:30:30.672995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Performance Analysis\nThe dataset consists of 1,338 entries with features including age, sex, BMI, number of children, smoker status, region, and insurance charges. The analysis compares three regression models: Linear Regression, Polynomial Regression, and K-Nearest Neighbors (KNN).","metadata":{"id":"1sF5MAQd_tyL"}},{"cell_type":"markdown","source":"#### Linear Regression\n\n- Mean Squared Error (MSE): 33,596,915.85\n- Mean Absolute Error (MAE): 4,181.19\n- R-squared (R²): 0.78\n\nLinear Regression shows a moderate performance with an R-squared value of 0.78, indicating that approximately 78% of the variance in insurance charges is explained by the model. The high MSE and MAE suggest that while the model captures a substantial amount of variance, there is room for improvement in prediction accuracy.","metadata":{"id":"Y3xaPOc-EmO6"}},{"cell_type":"markdown","source":"#### Polynomial Regression\n\n- Best Model Parameters: {'model__fit_intercept': True, 'poly__degree': 2, 'poly__include_bias': True}\n- Best Score: 24,447,087.44\n- Mean Squared Error (MSE): 20,712,805.99\n- Mean Absolute Error (MAE): 2,729.50\n- R-squared (R²): 0.87\n\nPolynomial Regression performs the best among the three models. With an R-squared value of 0.87, it explains approximately 87% of the variance in insurance charges. The lower MSE and MAE indicate a better fit and more accurate predictions compared to Linear Regression. The polynomial model effectively captures the non-linear relationships in the data.","metadata":{"id":"ws3RBJPYEpQX"}},{"cell_type":"markdown","source":"#### K-Nearest Neighbors (KNN)\n- Best Model Parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 5, 'model__weights': 'distance'}\n- Best Score: 40,301,203.76\n- Mean Squared Error (MSE): 37,984,876.27\n- Mean Absolute Error (MAE): 3,618.19\n- R-squared (R²): 0.76\n\nKNN shows moderate performance with an R-squared value of 0.76. While it performs reasonably well, its MSE and MAE are higher compared to Polynomial Regression. This suggests that KNN may not capture the data's complexity as effectively as the polynomial model but still provides a reasonable fit.","metadata":{"id":"prd4oV64GToV"}},{"cell_type":"markdown","source":"#### Summary\n\n- Polynomial Regression offers the best performance in terms of fitting the data and making accurate predictions, with the lowest MSE and MAE and the highest R-squared value.\n- Linear Regression is a good baseline model but underperforms compared to Polynomial Regression.\n- KNN provides decent predictions but does not match the accuracy of Polynomial Regression.","metadata":{"id":"xc8muoEADvBB"}},{"cell_type":"markdown","source":"#### Visualizations","metadata":{"id":"Xdv2-CytDduB"}},{"cell_type":"code","source":"plt.figure(figsize=(18, 12))\n\n# K-Nearest Neighbors (KNN) Plots\nplt.subplot(3, 3, 1)\nplt.scatter(y_test, knn_predictions, color='red', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('K-Nearest Neighbors')\n\nplt.subplot(3, 3, 2)\nsns.scatterplot(x=knn_predictions, y=y_test - knn_predictions, color='red', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('K-Nearest Neighbors Residuals')\n\n# Polynomial Regression Plots\nplt.subplot(3, 3, 4)\nplt.scatter(y_test, poly_predictions, color='green', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('Polynomial Regression')\n\nplt.subplot(3, 3, 5)\nsns.scatterplot(x=poly_predictions, y=y_test - poly_predictions, color='green', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('Polynomial Regression Residuals')\n\n# Linear Regression Plots\nplt.subplot(3, 3, 7)\nplt.scatter(y_test, pred_linear_reg, color='blue', alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='black')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('Linear Regression')\n\nplt.subplot(3, 3, 8)\nsns.scatterplot(x=pred_linear_reg, y=y_test - pred_linear_reg, color='blue', alpha=0.5)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel('Predicted Charges')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"tV4U-p5u5bxV","outputId":"44c6a4f4-c6e5-4d87-9a4f-3c871873a154","execution":{"iopub.status.busy":"2024-08-07T09:30:30.675322Z","iopub.execute_input":"2024-08-07T09:30:30.675625Z","iopub.status.idle":"2024-08-07T09:30:32.065337Z","shell.execute_reply.started":"2024-08-07T09:30:30.6756Z","shell.execute_reply":"2024-08-07T09:30:32.063953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn this analysis, we compared the performance of three models—Linear Regression, Polynomial Regression, and K-Nearest Neighbors (KNN)—for predicting insurance charges.\n\n- **Polynomial Regression** emerged as the top-performing model, with the highest R-squared value of 0.87. This indicates that it explains approximately 87% of the variance in insurance charges. Its lower Mean Squared Error (MSE) of 20,712,805.99 and Mean Absolute Error (MAE) of 2,729.50 reflect its superior predictive accuracy and ability to capture non-linear relationships in the data.\n\n- **K-Nearest Neighbors (KNN)** showed moderate performance with an R-squared value of 0.76. While it provides reasonable predictions, its MSE of 37,984,876.27 and MAE of 3,618.19 are higher compared to Polynomial Regression. This suggests that KNN may not capture the data's complexities as effectively but still offers a decent fit.\n\n- **Linear Regression** had an R-squared value of 0.78, explaining around 78% of the variance in insurance charges. Its MSE of 33,596,915.85 and MAE of 4,181.19 were higher compared to both Polynomial Regression and KNN. This indicates that while Linear Regression provides a baseline model, it is less effective at capturing the nuances of the data compared to the other models.\n\nOverall, Polynomial Regression provides the best fit and most accurate predictions for this dataset, making it the preferred choice for modeling insurance charges. Future work may involve exploring additional models or further tuning these algorithms to enhance prediction accuracy.","metadata":{"id":"Sn-NXwxi_NGD"}}]}